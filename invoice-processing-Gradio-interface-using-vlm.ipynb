{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10478928,"sourceType":"datasetVersion","datasetId":6488550},{"sourceId":10569225,"sourceType":"datasetVersion","datasetId":6540220},{"sourceId":10587270,"sourceType":"datasetVersion","datasetId":6552267}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install qwen-vl-utils gradio ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:06:51.041758Z","iopub.execute_input":"2025-01-31T17:06:51.042009Z","iopub.status.idle":"2025-01-31T17:07:03.478267Z","shell.execute_reply.started":"2025-01-31T17:06:51.041989Z","shell.execute_reply":"2025-01-31T17:07:03.477465Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting qwen-vl-utils\n  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\nCollecting gradio\n  Downloading gradio-5.14.0-py3-none-any.whl.metadata (16 kB)\nCollecting av (from qwen-vl-utils)\n  Downloading av-14.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (24.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (11.0.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from qwen-vl-utils) (2.32.3)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\nCollecting gradio-client==1.7.0 (from gradio)\n  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\nCollecting markupsafe~=2.0 (from gradio)\n  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\nRequirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\nRequirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (2024.9.0)\nRequirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.7.0->gradio) (14.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (3.4.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->qwen-vl-utils) (2.2.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\nDownloading gradio-5.14.0-py3-none-any.whl (57.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading av-14.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, av, starlette, qwen-vl-utils, safehttpx, gradio-client, fastapi, gradio\n  Attempting uninstall: markupsafe\n    Found existing installation: MarkupSafe 3.0.2\n    Uninstalling MarkupSafe-3.0.2:\n      Successfully uninstalled MarkupSafe-3.0.2\nSuccessfully installed av-14.1.0 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.14.0 gradio-client-1.7.0 markupsafe-2.1.5 python-multipart-0.0.20 qwen-vl-utils-0.0.10 ruff-0.9.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import json\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import Image\nimport pytesseract\nimport torch\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:07:03.479159Z","iopub.execute_input":"2025-01-31T17:07:03.479473Z","iopub.status.idle":"2025-01-31T17:07:23.389633Z","shell.execute_reply.started":"2025-01-31T17:07:03.479449Z","shell.execute_reply":"2025-01-31T17:07:23.388773Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Model Class","metadata":{}},{"cell_type":"code","source":"\nclass InvoiceProcessor:\n    \"\"\"\n    A class to handle processing invoices using the Qwen2-VL-2B-Instruct model.\n    This class provides functionality to extract information from invoice images and text prompts.\n    \"\"\"\n\n    def __init__(self, model_name=\"Qwen/Qwen2-VL-2B-Instruct\", device=None):\n        \"\"\"\n        Initialize the InvoiceProcessor class.\n\n        Args:\n            model_name (str): The name of the model to be loaded.\n            device (str): Device to load the model on ('cuda', 'cpu', or None for auto-detection).\n        \"\"\"\n        min_pixels = 256 * 28 * 28  # Minimum size for visual tokens\n        max_pixels = 1280 * 28 * 28  # Maximum size for visual tokens\n        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model_name = model_name\n        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n            self.model_name, torch_dtype=torch.float16, device_map=\"auto\").eval()\n        self.processor = AutoProcessor.from_pretrained(self.model_name ,min_pixels=min_pixels,max_pixels=max_pixels)\n        # Initialize the OCR model (with pretrained weights)\n        self.response = \"\"\n\n\n    def process_image(self, image_path):\n        \"\"\"\n        Prepares the inputs for the model.\n\n        Args:\n            image_path (str): Path to the invoice image.\n            prompt (str): Prompt guiding the model's behavior.\n\n        Returns:\n            dict: Prepared inputs ready for the model.\n        \"\"\"\n        torch.cuda.empty_cache()\n        processed_image = self.preprocess_image(image_path)\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are an advanced model tasked with extracting structured data from invoices. An invoice includes the following key sections. Extract the data accurately and return it in the specified JSON format.\n\n                Key Sections to Extract:\n                               \n                Invoice Information: Extract the information for invoice like invoice number, invoice date, Due date etc.\n                Business: Extract the Business name, Business address,business mail,business phone number,business GSTIN,business PAN etc of the issuing business.\n                Customer: Extract the Customer name,Customer address,business mail,business phone number,business GSTIN, customer PAN etc of the billed customer.\n                Product/Service: Extract the details of each billed item/servic include all subfield mentioned.\n                shipment: Extract details related to shipment.\n                Bank detail: extract bank related information.\n                Taxes Information: Extract details for each Taxable value,Central taxes,State taxes(Total Tax Amount,Tax Rate) .\n                Total Amount Information: Extract all kind of amount information.\n                if specific section or parameter not present skip those.\n            \"\"\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": processed_image},\n                    {\"type\": \"text\", \"text\": f\"Extract all the important and relevant information from this image\"},\n                ],\n            },\n        ]\n\n        # Prepare the chat input and vision info\n        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt= False)\n        image_inputs, video_inputs = process_vision_info(messages)\n\n        # Prepare and return model inputs\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs.to(self.device)\n        # Inference: Generation of the output\n        generated_ids = self.model.generate(**inputs, max_new_tokens= 1200, temperature = 0.01,top_p=1.0 )\n        generated_ids_trimmed = [\n           out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n           ]\n        output_text = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n           )\n        del inputs, generated_ids, generated_ids_trimmed\n        torch.cuda.empty_cache()\n\n        cleaned_text = ''.join(output_text).strip(\"```json\\n\")\n        self.response = cleaned_text\n\n        return cleaned_text\n\n    def parse_json(self):\n          \"\"\"\n               Parse a JSON string into a flat dictionary of key-value pairs, ignoring intermediate keys if the value contains key-value pairs.\n          \"\"\"\n          parsed_dict = {}\n          json_string = self.response\n\n          def recursive_parser(data, parent_key=\"\"):\n              if isinstance(data, dict):\n                 for key, value in data.items():\n                     # If the value is a dictionary, recurse without appending the key\n                     if isinstance(value, dict):\n                       recursive_parser(value, parent_key)\n                     else:\n                        # Create a new key by appending the parent key (if exists)\n                        new_key = f\"{parent_key}.{key}\" if parent_key else key\n                        recursive_parser(value, new_key)\n              elif isinstance(data, list):\n                 for index, item in enumerate(data):\n                     # Include index in key for list items\n                     new_key = f\"{parent_key}[{index}]\"\n                     recursive_parser(item, new_key)\n              else:\n                 # For leaf nodes, store the value in the dictionary\n                 parsed_dict[parent_key] = data\n\n          # Parse the JSON string\n          try:\n              json_data = json.loads(json_string)\n              recursive_parser(json_data)\n          except json.JSONDecodeError as e:\n                 print(\"JSONDecodeError: Falling back to regex-based parsing.\")\n\n                 # Fallback: Parse using regex\n                 # Regex pattern to capture key-value pairs inside quotes\n                 pattern = r'\"([^\"]+)\"\\s*:\\s*(\"[^\"]+\"|\\d+|\\[.*?\\]|\\{.*?\\})'\n                 data = json_string\n                 matches = re.findall(pattern, data)\n                 extracted_data = {}\n\n                 for key, value in matches:\n                       # Remove surrounding quotes from keys and values\n                       key = key.strip('\"')\n                       value = value.strip('\"')\n\n                       # Attempt to interpret lists or nested JSON-like values\n                       if value.startswith(\"{\") and value.endswith(\"}\"):\n                          try:\n                              extracted_data[key] = json.loads(value)\n                          except json.JSONDecodeError:\n                              extracted_data[key] = value  # Treat as raw string if invalid JSON\n                       elif value.startswith(\"[\") and value.endswith(\"]\"):\n                          try:\n                              extracted_data[key] = json.loads(value)\n                          except json.JSONDecodeError:\n                              extracted_data[key] = value.split(\",\")  # Fallback: Treat as list of strings\n                       else:\n                            extracted_data[key] = value  # Treat as plain string or number\n\n                 return extracted_data\n\n          return parsed_dict\n          \n    def preprocess_image(self,image_path, max_resolution=(1024, 1024)):\n      \"\"\"\n        Resize the image to fit within the max_resolution while maintaining aspect ratio.\n      \"\"\"\n      img = Image.open(image_path).convert(\"RGB\")\n      original_width, original_height = img.size\n      #print(f\"Original Image Size: {original_width}x{original_height} pixels\")\n\n      if original_width > max_resolution[0] or original_height > max_resolution[1]:\n         img.thumbnail(max_resolution, Image.Resampling.LANCZOS)     \n      return img    \n\n    def image_QA_processing(self, image_path, user_input):\n        \"\"\"\n        Prepares the inputs for the model.\n\n        Args:\n            image_path (str): Path to the invoice image.\n            prompt (str): Prompt guiding the model's behavior.\n\n        Returns:\n            dict: Prepared inputs ready for the model.\n        \"\"\"\n\n        torch.cuda.empty_cache()\n        processed_image = self.preprocess_image(image_path)\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"You are a highly capable Vision-Language Model (Qwen2-2B VLM) specializing in invoice data extraction. \n                Your task is to accurately answer user questions based on the content of an invoice image.\n                -If value not found return \"`No Data Present`\"\n                             \n                  \"\"\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\", \"image\": processed_image},\n                    {\"type\": \"text\", \"text\": f\"{user_input}\"},\n                ],\n            },\n        ]\n\n        # Prepare the chat input and vision info\n        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt= False)\n        image_inputs, video_inputs = process_vision_info(messages)\n\n        # Prepare and return model inputs\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs.to(self.device)\n        # Inference: Generation of the output\n        generated_ids = self.model.generate(**inputs, max_new_tokens= 1200, temperature = 0.01,top_p=1.0 )\n        generated_ids_trimmed = [\n           out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n           ]\n        output_text = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n           )\n        del inputs, generated_ids, generated_ids_trimmed\n        torch.cuda.empty_cache()\n        text_string = output_text[0]  # Access the string from the list\n        try:\n          if \"system\\n\" in text_string:\n             # If \"system\\n\" is present, split and extract\n             parts = text_string.split('\\n')\n             extracted_text = parts[1] if len(parts) > 1 else \"\"  # Handle cases with no text after \"system\\n\"\n             return extracted_text\n          else:\n            # If \"system\\n\" is not present, return the original text\n            extracted_text = text_string\n            return extracted_text\n        except:\n            return output_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:36:22.956343Z","iopub.execute_input":"2025-01-30T08:36:22.956915Z","iopub.status.idle":"2025-01-30T08:36:22.975170Z","shell.execute_reply.started":"2025-01-30T08:36:22.956889Z","shell.execute_reply":"2025-01-30T08:36:22.974332Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Gradio App","metadata":{}},{"cell_type":"code","source":"import gradio as gr\n# ... (other imports and functions)\n\nVLM_obj = InvoiceProcessor()\ntorch.cuda.empty_cache()\n\ndef parsing_image(image,data_state):\n    torch.cuda.empty_cache()\n    VLM_obj.process_image(image)\n    data_state = VLM_obj.parse_json()\n    #data = data_state\n    \n    return \"Image successfully Processed!\",data_state\n\ndef answer_question(image,question):\n    torch.cuda.empty_cache()\n    answer = VLM_obj.image_QA_processing(image,question)\n    return answer\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        # Column 1: Image Upload Section\n        with gr.Column():\n            # ... (image upload components)\n            gr.Markdown(\"### Upload an Image\")\n            image_input = gr.Image(label=\"Upload Image\", type=\"filepath\")\n            image_output = gr.Textbox(label=\"Image Status\")\n            image_button = gr.Button(\"Process\")\n            data_state = gr.State([])  # Define data_state here\n            image_button.click(parsing_image, \n                               inputs=[image_input, data_state], \n                               outputs=[image_output, data_state])\n\n        # Column 2: Tabs Section\n        with gr.Column():\n            # ... (other components)\n            with gr.Tabs():\n                with gr.Tab(\"Fields\"):\n                    # ... (Fields tab code)\n                    # Access and use data_state here, e.g.:\n                    \n                    # Example: Display the data in a JSON component\n                    #json_display = gr.JSON(label=\"Extracted Data\")\n                    data_state.change(lambda x: x, inputs=data_state, outputs=None) \n                    \n                    with gr.Row():\n                       field_name = gr.Textbox(label=\"Field Name\", autofocus=True)\n                       field_value = gr.Textbox(label=\"Field Value\")\n\n                    def add_field(data_state, new_field_name, new_field_value):\n                       if not new_field_name or not new_field_value:  # Check if either field is blank\n                          gr.Warning(\"Field name or value cannot be blank\")\n                          return data_state, gr.update(value=\"\", visible=True), gr.update(value=\"\", visible=True) \n                       elif new_field_name in data_state:\n                          gr.Warning(\"Field already existed\")\n                          return data_state, \"\", \"\"\n                       else:\n                          data_state[new_field_name] = new_field_value\n                          return data_state, \"\", \"\"\n\n                    add_field_btn = gr.Button(\"Add Field\")\n                    add_field_btn.click(add_field, [data_state, field_name, field_value], [data_state, field_name, field_value])\n\n                    @gr.render(inputs=data_state)\n                    def render_fields(data_dict):\n                        gr.Markdown(f\"### Fields ({len(data_dict)})\")\n\n                        for field_name, field_value in data_dict.items():\n                           with gr.Row():\n                              # Make Textboxes editable\n                              name_textbox = gr.Textbox(field_name, show_label=False, container=False, label=\"Field Name\", interactive=True,scale=2)\n                              value_textbox = gr.Textbox(field_value, show_label=False, container=False, label=\"Field Value\", interactive=True,scale=2)\n\n                              \n                             \n                              delete_btn = gr.Button(\"Delete\", scale=0, variant=\"stop\")\n                              update_btn = gr.Button(\"Update\", scale=0, variant=\"primary\")\n\n                              def delete_field(data_state=data_dict, field_name=field_name):\n                                  del data_dict[field_name]\n                                  return data_dict\n\n                              delete_btn.click(delete_field, None, [data_state])\n\n                              def create_edit_handler(original_field_name, data_state=data_dict):\n                                 def edit_field(new_name, new_value):\n                                    if not new_name or not new_value:\n                                       gr.Warning(\"Field Name or Value cannot be blank!\")\n                                       return gr.update(value=original_field_name),gr.update(value=data_dict[original_field_name]),data_dict\n                                    elif new_name in data_dict and original_field_name != new_name:\n                                       gr.Warning(\"Field Name already Exist!\")\n                                       return gr.update(value=original_field_name),gr.update(value=data_dict[original_field_name]),data_dict\n                                    elif original_field_name in data_dict:\n                                       if original_field_name != new_name or data_dict[original_field_name] != new_value:\n                                           keys = list(data_dict.keys())\n                                           values = list(data_dict.values())\n                                           for i in range(len(keys)):\n                                               if original_field_name == keys[i]:\n                                                   keys[i] = new_name\n                                                   values[i] = new_value\n                                                   break\n\n                                           # Update the dictionary in-place\n                                           data_dict.clear()\n                                           data_dict.update(dict(zip(keys, values)))\n\n                                           # data_dict = dict(zip(keys, values))\n                                           return gr.update(value=new_name), gr.update(value=new_value), data_dict\n                                    \n                                 return edit_field\n\n                              # ... (Inside render_fields function)\n                              update_btn.click(create_edit_handler(field_name), [name_textbox, value_textbox], [name_textbox,value_textbox,data_state])\n\n\n                        return []\n\n                    # Add a button to get the updated data\n                    get_data_btn = gr.Button(\"Get Data\")\n\n                    def get_data(data_state):\n                      return data_state  # Return the current data_state\n\n                    get_data_btn.click(get_data, inputs=[data_state], outputs=[gr.JSON(label=\"Updated Data\")])    \n\n                with gr.Tab(\"Questions-Answer\"):\n                    # ... (Questions-Answer tab code)\n                    # Access data_state if needed in this tab as well\n                    # ...\n                    gr.Markdown(\"#### Ask a Question\")\n                    question_input = gr.Textbox(label=\"Your Question\")\n                    answer_output = gr.Textbox(label=\"Answer\")\n                    question_button = gr.Button(\"Get Answer\")\n                    question_button.click(answer_question, inputs=[image_input,question_input], outputs=answer_output)\n\ndemo.launch(debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T08:38:16.818727Z","iopub.execute_input":"2025-01-30T08:38:16.819019Z","execution_failed":"2025-01-30T09:52:25.661Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://bac15a14419e67441d.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://bac15a14419e67441d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 625, in process_events\n    response = await route_utils.call_process_api(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2044, in process_api\n    result = await self.call_function(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1591, in call_function\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 883, in wrapper\n    response = f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/renderable.py\", line 75, in apply\n    self.fn(*args, **kwargs)\n  File \"<ipython-input-6-1adec6a0f123>\", line 66, in render_fields\n    for field_name, field_value in data_dict.items():\nAttributeError: 'list' object has no attribute 'items'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 625, in process_events\n    response = await route_utils.call_process_api(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n    output = await app.get_blocks().process_api(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2044, in process_api\n    result = await self.call_function(\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1591, in call_function\n    prediction = await anyio.to_thread.run_sync(  # type: ignore\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n    return await get_asynclib().run_sync_in_worker_thread(\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n    return await future\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n    result = context.run(func, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 883, in wrapper\n    response = f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/gradio/renderable.py\", line 75, in apply\n    self.fn(*args, **kwargs)\n  File \"<ipython-input-6-1adec6a0f123>\", line 66, in render_fields\n    for field_name, field_value in data_dict.items():\nAttributeError: 'list' object has no attribute 'items'\n","output_type":"stream"},{"name":"stdout","text":"JSONDecodeError: Falling back to regex-based parsing.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1777: UserWarning: A function returned too many output values (needed: 0, returned: 1). Ignoring extra values.\n    Output components:\n        []\n    Output values returned:\n        [{'Invoice Number': '783944', 'Invoice Date': '11/5/2019', 'Due Date': '12/5/2019', 'Business Name': 'Bag of Beans Cafe & Restaurant Inc.', 'Business Address': '117 Aguinaldo Highway Crossing Mendez', 'Business Mail': 'West Tagaytay City Cavite 4120', 'Business Phone Number': '008-117-738-000', 'Business GSTIN': 'IN 008 117 738 000', 'Business PAN': 'IN 008 117 738 000', 'Customer Name': 'Guest 3', 'Customer Address': '117 Aguinaldo Highway Crossing Mendez', 'Customer PAN': 'IN 008 117 738 000', 'SubTotal': '610', 'PreTax': '544', 'Serv Charge (10%)': '54', '12% VAT': '65', 'Total Tax Amount': '0', 'Tax Rate': '0', 'Amount Due': '664'}]\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null}]}